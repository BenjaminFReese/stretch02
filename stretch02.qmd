---
title: "Stretch 02"
subtitle:"Predicting Why Civil Servants Exit"
author: "Benjamin Reese"
format: html
self-contained: true
---

## Setup

```{r packages and data loading, warning=TRUE, message=FALSE}
## Packages
library(tidyverse)
library(tidymodels)
library(lubridate)
library(readr)
library(patchwork)
library(ggjoy)
library(vip)

## Data Loading
sep_data <- read.csv("SEPDATA_FY2005-2009.TXT")
load("~/Fall 2022/Data Science/stretch02/data/fed.RData")
app_data <- read_csv("data/EAD+2.0+quarter+101019.csv")
```

### (a)

#### Why Do Federal Employees Leave the Civil Service?

I am loading and joining three datasets together. The main dataset, that includes the variable I want to predict is `sep_data`. This dataset includes information on nearly every federal employee, covering most non-intelligence community agencies, who left federal service from 2005-2009. This dataset can be found [here](https://www.opm.gov/data/index.aspx). The goal of this analysis is to predict why federal employees leave the federal workforce. I construct the classification as quit or not quit for simplicity. The other two datasets are for factors beyond what OPM tracks that may contribute to people leaving the federal workforce. The first of the supplementary datasets includes economic variables like inflation and a variable for presidential approval prepared by Mike Bailey for his introductory statsitics textbook. It can be found [here](https://global.oup.com/us/companion.websites/9780199981946/stu/ch6/data_sets/). The second of the supplementary datasets was compiled by the Executive Approval Project and covers presidential approval. It can be found [here](http://www.executiveapproval.org/datasets-1/). The end result of the analysis will be a model that predicts why a civil servant quits versus leaving for another reason such as retirement or being fired. 

Types of non-quitting civil servant exit includes retirements and Reductions In Force are mostly controllable or more easily predicted by agencies based on the age of their employees and their internal staffing decisions. Quitting, though, can seriously disrupt an agency's functionality and is not as easily controlled or predictable by federal managers. Hopefully, this model will give some indication about why employees quit and show the important factors that predict quitting. In sum, this model hopes to answer the question: why do federal employees leave the civil service?

### Data Wrangling

The code below reads in the three datasets, creates some variables of interest and joins them all together.

```{r cleaning and joining, warning=FALSE,message=FALSE}
## Loading in data on when federal employees quit, creating education and quit variables
sep_data <- sep_data %>%
  mutate(quarter = str_replace_all(EFDATE, "(.{4})(.*)", "\\1-\\2"),
         quarter = quarter(ym(quarter), type = "year.quarter"),
         separation = if_else(SEP == "SC", "Quit", "Not Quit"),
         EDLVL = as.numeric(EDLVL)) %>%
  filter(!is.na(EDLVL) & EDLVL != "**")%>%
  mutate(education = case_when(
           EDLVL < 4 ~ "Less Than High School",
           EDLVL == 4 ~ "High School or GED",
           EDLVL == 5 | EDLVL == 6 | EDLVL == 7 | EDLVL == 8 | EDLVL == 9 |
           EDLVL == 10 | EDLVL == 11 | EDLVL == 12 ~ "Less than Bachelors",
           EDLVL == 13 ~ "Bachelors",
           EDLVL > 13 ~ "Graduate")) %>%
  select(separation, education, quarter, SALARY, LOS)

## dropping unused variables, creating a unified presidents and party variable, and filtering to only quarters
## included in the civil servant exit data
econ_data <- dta %>%
  select(-Democrat, -pres1, -pres2, -pres3, -pres4, -pres5, -pres6, -pres7, 
         -pres8, -pres9, -pres10, -pres11, -pres12, -pres13, -pres14, -pres15) %>%
  mutate(admin = case_when(
    president == 1 ~ "Obama",
    president == 2 | president == 3 ~ "Eisenhower",
    president == 4 ~ "H.W. Bush",
    president == 5 | president == 6 ~ "W. Bush",
    president == 7 ~ "Carter",
    president == 8 ~ "Kennedy-Johnson",
    president == 9 ~ "Johnson",
    president == 10 ~ "Nixon-Ford",
    president == 11 ~ "Nixon",
    president == 12 | president == 13 ~ "Reagan",
    president == 14 | president == 15 ~ "Clinton"),
  party = case_when(
    president_party == 2 ~ "R",
    president_party == 1 ~ "D"),
  quarter = quarter(DATE,type = "year.quarter")) %>%
    select(-president, -president_party, -DATE, -ygap, -Quarters)

## filtering to only include the US, dropping unused variables and renaming
app_data <- app_data %>%
  filter(Country == "United States") %>%
  select(qtr, Approval_Smoothed) %>%
  mutate(approval = Approval_Smoothed,
    quarter = as.numeric(str_replace_all(qtr, 'q', "."))) %>%
  select(-Approval_Smoothed, -qtr)

## joining the three datasets all together into one unified tibble
sep <- sep_data %>%
  left_join(econ_data, by="quarter") %>%
  left_join(app_data, by = "quarter") %>%
  mutate(quarter = as.character(quarter)) %>%
  janitor::clean_names()
```

### (b)

```{r splitting, warning=TRUE, message=FALSE}
## Setting Seed
set.seed(20201124)

## Splitting the Sample
split <- initial_split(sep, prop = 0.7)

## Training and Testing
sep_train <- training(split)
sep_test <- testing(split)
```

### (c)

```{r exploratory data analysis, warning=TRUE, message=FALSE}
## Displaying More Places In Numbers
options(scipen = 999999)

## Exploratory Data Analysis

## Line Plot of the # Employees Quitting
m2 <- sep_train %>%
  filter(separation == "Quit") %>%
  group_by(quarter) %>%
  count(separation=="Quit") %>%
  ggplot(aes(x=quarter, y=n)) +
  geom_line(lwd=1, col="light blue") +
    geom_point(col="blue") +
  theme_minimal() +
  labs(title = "When Civil Servants Exit", 
       subtitle = "Federal Employees Quit Most in Third Quarters",
       x=NULL, y="# Quitting")

## Line Plot of the Percentage of Employees Leaving That Quit
m1 <- sep_train %>%
  mutate(separation = if_else(separation == "Quit", 1, 0)) %>%
  group_by(quarter) %>%
  summarise(percent_quit = mean(separation)) %>%
  mutate(percent_quit=percent_quit*100) %>%
  ggplot(aes(x=quarter, y=percent_quit)) +
  geom_line(lwd=1, col="light blue") +
   geom_point(col="blue") +
  theme_minimal() +
  lims(y=c(33,43))+
  labs(x="Fiscal Year Quarter", y="Percent Exits Who Quit", 
       caption = "Data Source: Office of Personnel Management")
## Using patchwork to display together
m2 / m1

## Relationship Between Salary And Quitting
sep_train %>%
  group_by(separation) %>%
  ggplot(aes(x=salary, y=separation)) +
  geom_violin(scale="area", col="blue", fill="light blue") +
  theme_minimal() +
  coord_flip() +
  labs(y=NULL, x="Salary ($)", 
       title="Distribution of Salary For Those Who Quit Vs Leave For Other Reasons",
       subtitle = "Lower Income Employees More Likely to Quit")

## Relationship Between Presidential Approval And Quitting
sep_train %>%
  group_by(separation) %>%
  ggplot(aes(x=approval, y=separation, fill=separation)) +
  geom_joy() +
  theme_minimal() +
  labs(y=NULL, x="Approval Rating (%)", 
       title="Presidential Approval and Federal Employee Exit",
       subtitle = "More Employees Quit When Presidential Approval Is Low",
       fill=NULL)

## Relationship Between Fed Funds Rate And Quitting
sep_train %>%
  group_by(separation) %>%
  ggplot(aes(x=lag_fedfunds, y=separation, fill=separation)) +
  geom_joy() +
  theme_minimal() +
  labs(y=NULL, x="Lagged Fed Funds Rate", 
       title="Federal Interest Rates and Employee Exit",
       subtitle = "More Employees Leave When Interest Rates Are High",
       fill=NULL)

## Party And Quitting
sep_train %>%
  mutate(separation = if_else(separation == "Quit", 1, 0)) %>%
  group_by(party) %>%
  summarise(percent_quit = mean(separation))
  

```

The data analysis above explores a few possible variables that may be important in predicting why federal employees quit. The first two plots simply plot the number and proportion of quitting employees over the nearly 5 years included in the dataset. 

The violin plot below the line graphs shows that the majority of quitters are in low income positions, meaning the people with higher pay, and probably more prestigious or professional occupations, are less likely to quit than lower-income employees.

Perhaps expectedly, federal employees seem to be more likely to quit when presidential approval rating is lower. As the president is the Chief Executive Officer of the federal bureaucracy, it makes sense that a disliked president will see more of their employees leaving than a liked president. It is important to remember that this data mostly only for 1 president, though: George W. Bush.

The joyplot uses the Federal Reserve Funds rate as a proxy for economic performance. The Federal Reserve usually increases interest rates to slow economic growth in order to stem inflation and maintain normative location on the Phillip's Curve. Times with high Fed Funds rates can be used as an imperfect economic indicator. The plot shows that more civil servants quit when Fed Funds rates are high. The full dataset includes variables for inflation, so it will be interesting to see how important economic performance is for predicting federal employee exit.

Finally, the table above shows that Republicans, in this case only George W. Bush, saw a higher percentage of employees leaving as quitters than Obama.

### (d)

Overall Accuracy and the Recall/Sensitivy/True Positive rate are the most important error metrics for this analysis because the goal is to accurately predict an event. Incorrect predictions are the most costly form of error because not preparing for a large sudden civil servant exit could cause large efficiency losses in the federal service. Thus, a false negative is more damaging than a false positive. A false positive, or expecting a federal employee to quit, may still carry large costs if the prediction is so incorrect that it causes agencies to hire unneeded employees, but, overall, with the goal being to reduce uncertainty in the exiting of civil servants, false negatives are far more costly. In sum, the costs of false negatives, and a low sensitivity/recall makes the model less useful because it is not fulfilling its role in reducing uncertainty in federal employee exit. While I will select the best model based on overall accuracy, I will also evaluate the final model in terms of recall.

## Models

### (a) 

I define here three candidate models with two different pre-processings. Their exact specifications are below.

The outcome variable to be classified is reason for separation, and it will be down-sampled in one recipe. The predictor variables are: level of education - which will be pre-processed into dummy variables, fiscal quarter - which will be pre-processed into dummy variables, employee salary - which will be normalized, the Federal Reserve Funds rate - which will be normalized, debt-to-GDP ratio - which will be normalized, expenditure-to-GDP ratio - which will be normalized, inflation -  which will be normalized, presidential administration - which will be made into dummy variables, party of president - will also be made into dummy variables, and presidential approving rating - which will be normalized. Since there are a variety of different units and, for example, the salary and inflation variables are wildly skewed, all of these predictors need normalization during pre-processing. I will also drop variables without much variance. I will also turn the character variables into dichotomous predictors for one of the recipes. The exact pre-processing for each variable can be found below:


```{r 2 recipes, warning=TRUE, message=FALSE}
## First Recipe
sep_rec_1 <-
  recipe(separation ~ ., data = sep_train) %>%
  themis::step_downsample(separation) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_nzv(all_numeric_predictors())

## Second Recipe
sep_rec_2 <-
  recipe(separation ~ ., data = sep_train) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_nzv(all_predictors())                                             

## Setting up folds for cross-validation
folds <- vfold_cv(sep_train, folds=10)
```

The three candidate models considered here are a CART model, a logistic regression model, and a random forest model. 

A CART, or classification and regression tree, model is a predictive model that predicts an outcome variable's values based on a set of predictors. The output of the CART model is a decision tree where each fork is a split in a predictor variable and each end node contains a prediction for the outcome variable. A logistic model, specifically as used in machine learning models, is a classification algorithm that predicts the probability of certain classifications based on some dependent variables. Finally, a random forest model is an algorithm that can be used both in classification and regression problems, and I would use it for classification. Random forest models build decision trees on different samples and take a majority vote for classification. The exact model specifications can be found below. The random forest model will include hyper-parameter tuning for the number of trees.

### CART Model Specification and Workflow

```{r cart model, warning=TRUE, message=FALSE}
## Creating the CART Model
cart_mod <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

## CART Workflow
cart_wf <- workflow() %>%
  add_recipe(sep_rec_1) %>%
  add_model(cart_mod)
```

### Random Forest Model Specification and Workflow

```{r rd model, warning=TRUE, message=FALSE}
## Creating Grid for Hyperparameter Tuning
rf_grid <- grid_regular(
  mtry(range = c(10, 30)),
  min_n(range = c(2, 8)),
  levels = 5)

## Random Forest Model
rf_mod <- rand_forest(mtry = tune(), trees = 1000, min_n = tune()) %>%
  set_engine("ranger") %>%
  set_mode("classification")

## Random Forest Workflow
rf_wf <- workflow() %>%
  add_recipe(sep_rec_2) %>%
  add_model(rf_mod)

## Tuning Hyperparameters
rf_class <- rf_wf %>%
  tune_grid(tune_wf, resamples = folds, grid = rf_grid)

```

### Logisitic Regression Model Specification and Workflow

```{r logistic, warning=TRUE, message=FALSE}
## Creating the Logistic Regression Model
log_mod <-
  logistic_reg() %>%
  set_engine(engine = "glm") %>%
  set_mode(mode = "classification")

## Logistic Regression Workflow
log_wf <- workflow() %>%
  add_recipe(sep_rec_1) %>%
  add_model(log_mod)
```

## 3

In this section, I implement the CART model above to try to predict the reasons why civil servants exit.

```{r implementation, warning=TRUE, message=FALSE}
## Creating the CART Model
cart_mod <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

## CART Workflow
cart_wf <- workflow() %>%
  add_recipe(sep_rec_1) %>%
  add_model(cart_mod)

## Estimating the Model
cart_fit <- cart_wf %>%
  fit_resamples(resamples = folds)

## Selecting Best Model
cart_fit_best <- cart_fit %>%
  select_best(cart_fit, metric = "accuracy")

## Finalizing Workflow for Best Model
cart_final <- cart_wf %>%
  tune::finalize_workflow(cart_fit_best) %>%
  parsnip::fit(data = sep_train)

```

## 4

In this section, I generate a confusion matrix, find the accuracy, find the recall, and visualize the results with a decision tree in order to interpret the effectiveness of the model.

```{r results, warning=TRUE, message=FALSE}
## The Predictions
predictions <- bind_cols(
  sep_test,
  predict(object = cart_final, new_data = sep_test),
  predict(object = cart_final, new_data = sep_test, type = "prob")
) %>%
  mutate(separation = as.factor(separation))

## The Predictions
select(predictions, separation, starts_with(".pred"))

## Confusion Matrix
conf_mat(data = predictions, truth = separation, estimate = .pred_class)

## Accuracy
accuracy(data = predictions, truth = separation, estimate = .pred_class)

## Recall/Sensitivity
recall(data = predictions, truth = separation, estimate = .pred_class)

## Create A Tree
rpart.plot::rpart.plot(x = cart_final$fit$fit$fit)

```

### (a)

Overall, the best model had an accuracy of $\approx .688$ and a recall rate of $\approx .53$. Recall, or how often the classifier is correct when there is an event was quite low. Considering the outcome variable, "quit" or "not quit", was dichotomous, a coin flip could give the correct result 50% of the time, and the recall value is not much higher. The accuracy, though, is higher and the model is nearly 70% accurate. 

In the context of the application, the results show that the length of service (los) is the main, and only variable that causes a split in the decision tree. This means that the largest predictor of whether an employee will quit or not is if the employee stays for longer than .43 years. Civil servants that quit seem to be more likely to do so early in the term of their service. The recommendation from this model would be to, perhaps obviously, try to keep employees initially for nearly half a year, and then that employee could spend much of their career with the agency or the agency can deciede when they leave. Taking into account the not perfect error metrics, agencies should focus efforts on appealing to recently hired employees in order to reduce uncertainty in the civil service.

### (b) 

The biggest weakness of this model is that I do not have data on the employees that did not leave for some reason, so there is no counter-factual to compare too. I also only have data for 2005-2009 which is highly limited. A better construction the dependent variable of interest, as I discuss below, would benefit the accuracy and better capture the intention of the model. This can be remedied through the use of a larger source of data that includes bureaucrats who did not separate, and then try to predict quitting when compared to continuing to work when the model now predicts quitting versus leaving for other reasons.

For feature engineering decisions, transforming some more variables during pre-processing may improve the accuracy and sensitivity of the model. Taking the log of salary could better account for outliers and the fact that some civil servants have vastly higher salaries than the mean. Second, creating new variables from existing variables, like using pay-grade to classify an employee's managerial status within the bureaucracy. It is possible that higher level civil servants, who probably have greater exit options, may be more likely to quit. A categorical variable that captures different GS levels may increase the performance of this model.  

For modeling decisions, I would first want to implement a random forest model. A random forest model, which can have better predictive power than a CART model, could increase the accuracy and recall of my model. I would also consider using a logistic regression model that could better predict the probability of quitting. 

Finally, part of both feature selection and modeling decisions, defining my dependent variable as length of service before quitting, a continuous variable, would allow the use of non-classifications algorithms like linear regression models utilizing LASSO or Elastic-Net methods. This opens up the slate of models that I can use which could lead to different, and more accurate, results.